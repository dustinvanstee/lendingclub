{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask with Rapids Multi GPU Demo [WIP]\n",
    "\n",
    "<img src=\"./nb_images/rapids_and_dask.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "This Demo will show how to use Dask + Rapids to scale jobs to multiple GPU's and multiple machines using AC922 servers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo Setup\n",
    "\n",
    "* start scheduler\n",
    "* start a single worker via CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Execute the functions below, they are needed for follow-on parts of the lab.  Note the **pgdf** function is a convenience function to display the GPU dataframe in a nice format for jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T13:55:04.986802Z",
     "start_time": "2019-10-08T13:55:04.979382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../utils/') \n",
    "\n",
    "#dask\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "\n",
    "# Rapids\n",
    "import cudf\n",
    "from cudf.dataframe import DataFrame as RapidsDataFrame\n",
    "# Dask -CUDF\n",
    "import dask_cudf\n",
    "\n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T13:51:45.804100Z",
     "start_time": "2019-10-08T13:51:45.801308Z"
    }
   },
   "outputs": [],
   "source": [
    "# [print gpu dataframe] helper function to print GPU dataframes \n",
    "def pgdf(gdf) :\n",
    "    display(gdf.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T13:51:46.300082Z",
     "start_time": "2019-10-08T13:51:45.805938Z"
    }
   },
   "outputs": [],
   "source": [
    "def time_command(cmd,repeat=1) :\n",
    "    avg_runtime = timeit.timeit(cmd, number=repeat)\n",
    "    return float(avg_runtime / repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T13:51:46.518939Z",
     "start_time": "2019-10-08T13:51:46.301673Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary to store results ..\n",
    "# example \"describe\" : {\"gpu\" : []}\n",
    "# TODO : make display results look better ..\n",
    "class COMPARE() :\n",
    "        ## Abstract Custom Implementations\n",
    "    def __init__(self) :\n",
    "        #nprint(\"Loading Data.  Overriding __init__ from dfutils\")\n",
    "        self.tests = []\n",
    "        self.gpu_results = {}\n",
    "        self.cpu_results = {}\n",
    "        self.df_shape = (0,0)\n",
    "        self.df_memory_gb = 0 \n",
    "\n",
    "    def add_result(self, test_name, gpu_result, runtime) :\n",
    "        if test_name not in self.tests :\n",
    "            self.tests.append(test_name)\n",
    "            self.gpu_results[test_name] = []\n",
    "            self.cpu_results[test_name] = []\n",
    "        \n",
    "        if(gpu_result == \"gpu\") :\n",
    "            self.gpu_results[test_name].append(runtime)\n",
    "        else :\n",
    "            self.cpu_results[test_name].append(runtime)\n",
    "            \n",
    "    def display_results(self) :\n",
    "        print(\"Dataframe size : {} {} GB\".format(self.df_shape, self.df_memory_gb))\n",
    "        print(\"{:<20} {:<20} {:<20} {:<20}\".format(\"test\", \"CPU(s)\", \"GPU(s)\", \"GPU Speedup\"))\n",
    "        for i in self.tests :\n",
    "            cpu_mean = sum(self.cpu_results[i]) / (len(self.cpu_results[i])+0.00001)\n",
    "            gpu_mean = sum(self.gpu_results[i]) / (len(self.gpu_results[i])+0.00001)\n",
    "            su = cpu_mean / (gpu_mean + .00001)\n",
    "            print(\"{:<20} {:<20.4f} {:<20.4f} {:<20.2f}\".format(i, cpu_mean, gpu_mean, su ))\n",
    "\n",
    "run_times = COMPARE()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T13:43:26.877680Z",
     "start_time": "2019-09-11T13:43:26.875136Z"
    }
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T13:51:47.587873Z",
     "start_time": "2019-10-08T13:51:46.721294Z"
    }
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "filename = \"../dataprep_common/loan_project_df.parquet.gzip\"\n",
    "\n",
    "# Expand data to highlight performance difference\n",
    "# 3 ~ 1GB dataset\n",
    "# 4 ~ 2GB dataset \n",
    "# ... etc\n",
    "# 7 - 16GB\n",
    "DATA_DOUBLE_FACTOR=7\n",
    "\n",
    "# Pandas dataframe\n",
    "loan_pdf = pd.read_parquet(filename)#  , names=ts_cols,dtype=ts_dtypes,skiprows=1)\n",
    "\n",
    "# Rapids Dataframe\n",
    "loan_rdf = cudf.read_parquet(filename)#  , names=ts_cols,dtype=ts_dtypes,skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T13:52:29.607869Z",
     "start_time": "2019-10-08T13:51:47.902129Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale up data to 10 million rows \n",
    "for i in range(DATA_DOUBLE_FACTOR) :\n",
    "    loan_pdf = pd.concat([loan_pdf,loan_pdf],axis=0)\n",
    "    loan_pdf = loan_pdf.reset_index().drop(\"index\",axis=1)\n",
    "    #loan_rdf = cudf.concat([loan_rdf,loan_rdf],axis=0)\n",
    "    #loan_rdf = loan_rdf.reset_index().drop(\"index\",axis=1)\n",
    "    #pgdf(loan_rdf.head())\n",
    "    #display(loan_pdf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T13:53:14.465409Z",
     "start_time": "2019-10-08T13:52:29.609967Z"
    }
   },
   "outputs": [],
   "source": [
    "loan_dadf = dd.from_pandas(loan_pdf,npartitions=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T13:54:03.901674Z",
     "start_time": "2019-10-08T13:54:03.899544Z"
    }
   },
   "outputs": [],
   "source": [
    "# [not working]\n",
    "#loan_dcdf = dask_cudf.from_dask_dataframe(loan_dadf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start up Dask Workers and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T02:12:32.285559Z",
     "start_time": "2019-10-04T02:12:32.282553Z"
    }
   },
   "outputs": [],
   "source": [
    "# conda activate powerai-1.6.1\n",
    "# dask-scheduler --port 8000\n",
    "# -->\n",
    "# distributed.scheduler - INFO - -----------------------------------------------\n",
    "# distributed.scheduler - INFO - Local Directory:    /tmp/scheduler-ry6vcxoy\n",
    "# distributed.scheduler - INFO - -----------------------------------------------\n",
    "# distributed.scheduler - INFO - Clear task state\n",
    "# /gpfs/gpfs_gl4_16mb/s4s004/vanstee/anaconda3/envs/powerai-1.6.1/lib/python3.6/site-packages/distributed/dashboard/core.py:72: UserWarning:\n",
    "# Port 8787 is already in use.\n",
    "# Perhaps you already have a cluster running?\n",
    "# Hosting the diagnostics dashboard on a random port instead.\n",
    "#   warnings.warn(\"\\n\" + msg)\n",
    "# distributed.dashboard.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: pip install jupyter-server-proxy\n",
    "# distributed.scheduler - INFO -   Scheduler at:  tcp://129.40.42.114:8786\n",
    "# distributed.scheduler - INFO -   dashboard at:                    :35599\n",
    "# \n",
    "# \n",
    "# --->\n",
    "# CUDA_VISIBLE_DEVICES=2 dask-worker localhost:8000 --nprocs 1 --nthreads 1\n",
    "# CUDA_VISIBLE_DEVICES=3 dask-worker localhost:8000 --nprocs 1 --nthreads 1\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T13:43:45.476443Z",
     "start_time": "2019-09-11T13:43:45.474153Z"
    }
   },
   "source": [
    "## Connect to client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T13:53:20.261934Z",
     "start_time": "2019-10-08T13:53:20.253810Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### Portions of this were borrowed and adapted from the\n",
    "#### cuDF cheatsheet, existing cuDF documentation,\n",
    "#### and 10 Minutes to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T13:53:31.576961Z",
     "start_time": "2019-10-08T13:53:21.420252Z"
    }
   },
   "outputs": [],
   "source": [
    "#from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "#cluster = LocalCUDACluster()  # runs on eight local GPUs\n",
    "client = Client('p10a114.pbm.ihost.com:8000')\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask CPU Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:18:45.251192Z",
     "start_time": "2019-09-24T20:17:38.111776Z"
    }
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "x = da.random.random((10000,10000,10), chunks=(1000,1000,5))\n",
    "y = da.random.random((10000,10000,10), chunks=(1000,1000,5))\n",
    "z = (da.arcsin(x) + da.arccos(y)).sum(axis=(1,2))\n",
    "\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T21:02:17.222129Z",
     "start_time": "2019-09-18T21:02:17.164282Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "#aaa = \"../rawdata/example.csv\"\n",
    "\n",
    "#gdf = dask_cudf.read_csv(aaa)  # wrap around many CSV files\n",
    "#print(gdf.dtypes)\n",
    "#gdf.head().to_pandas()\n",
    "#gdf['loan_amnt'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T19:11:53.254097Z",
     "start_time": "2019-09-25T19:05:59.384229Z"
    }
   },
   "outputs": [],
   "source": [
    "# ~100s runtime ... fix that pls\n",
    "a = np.random.rand(10000,10000)\n",
    "df_np = cudf.DataFrame()\n",
    "df_np = df_np.from_records(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask-CUDF  Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T13:53:14.684200Z",
     "start_time": "2019-10-08T13:53:14.467245Z"
    }
   },
   "outputs": [],
   "source": [
    "loan_ddf = dask_cudf.from_cudf(loan_rdf, npartitions=8).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T02:42:16.008917Z",
     "start_time": "2019-10-04T02:42:15.986263Z"
    }
   },
   "outputs": [],
   "source": [
    "loan_ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T02:42:17.257167Z",
     "start_time": "2019-10-04T02:42:16.816019Z"
    }
   },
   "outputs": [],
   "source": [
    "dir(loan_ddf)\n",
    "#loan_ddf[[\"loan_amnt\",\"annual_inc\"]].describe()\n",
    "#loan_ddf.sort_values(by='fico_range_high')\n",
    "grade_stats_ddf = loan_ddf.groupby('grade').agg({\"annual_inc\": [\"count\",\"mean\"], \"loan_amnt\": [\"count\",\"mean\"], \"dti\": [\"count\",\"mean\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T02:42:18.292192Z",
     "start_time": "2019-10-04T02:42:18.086971Z"
    }
   },
   "outputs": [],
   "source": [
    "grade_stats_ddf.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T02:42:47.586901Z",
     "start_time": "2019-10-04T02:42:19.347187Z"
    }
   },
   "outputs": [],
   "source": [
    "#(ddf[0].sum() + ddf[1].std()).compute()\n",
    "for i in range(20) :\n",
    "    grade_stats_ddf.compute().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost with Rapids [WIP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T13:56:02.998655Z",
     "start_time": "2019-09-18T13:53:16.096Z"
    }
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "#import GPUtil\n",
    "import time\n",
    "\n",
    "\n",
    "# Define the function to be executed on each worker\n",
    "def train(X, y, available_devices):\n",
    "    dtrain = xgb.dask.create_worker_dmatrix(X, y)\n",
    "    local_device = available_devices[xgb.rabit.get_rank()]\n",
    "    # Specify the GPU algorithm and device for this worker\n",
    "    params = {\"tree_method\": \"gpu_hist\", \"gpu_id\": local_device}\n",
    "    print(\"Worker {} starting training on {} rows\".format(xgb.rabit.get_rank(), dtrain.num_row()))\n",
    "    start = time.time()\n",
    "    xgb.train(params, dtrain, num_boost_round=500)\n",
    "    end = time.time()\n",
    "    print(\"Worker {} finished training in {:0.2f}s\".format(xgb.rabit.get_rank(), end - start))\n",
    "\n",
    "\n",
    "def main():\n",
    "    max_devices = 16\n",
    "    # Check which devices we have locally\n",
    "    # available_devices = GPUtil.getAvailable(limit=max_devices)\n",
    "    available_devices = [0,1]\n",
    "    # Use one worker per device\n",
    "    cluster = LocalCluster(n_workers=len(available_devices), threads_per_worker=4)\n",
    "    client = Client(cluster)\n",
    "\n",
    "    # Set up a relatively large regression problem\n",
    "    n = 100\n",
    "    m = 10000000\n",
    "    partition_size = 100000\n",
    "    X = da.random.random((m, n), partition_size)\n",
    "    y = da.random.random(m, partition_size)\n",
    "\n",
    "    xgb.dask.run(client, train, X, y, available_devices)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SnapML + Rapids [WIP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T13:56:02.999934Z",
     "start_time": "2019-09-18T13:53:18.811Z"
    }
   },
   "outputs": [],
   "source": [
    "### SnapML\n",
    "\n",
    "X = loan_norm_rdfloan_rdf2.to_pandas()\n",
    "y = loan_rdf['default'].to_pandas()\n",
    "\n",
    "from pai4sk.linear_model import Ridge as LRSNAP\n",
    "clf = LRSNAP(alpha=1.0)\n",
    "clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE\n",
    "\n",
    "\n",
    "#Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
    "#      normalize=False, random_state=None, solver='auto', tol=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T13:56:03.001189Z",
     "start_time": "2019-09-18T13:53:19.922Z"
    }
   },
   "outputs": [],
   "source": [
    "#from cudf.core import DataFrame, Series\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T14:22:52.241187Z",
     "start_time": "2019-08-26T14:22:52.044555Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df[[\"cats\",\"dogs\"]])\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T14:20:18.938366Z",
     "start_time": "2019-08-26T14:20:18.930519Z"
    }
   },
   "outputs": [],
   "source": [
    "print(uvals,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons Learned\n",
    "\n",
    "lessons learned ... data MUST be clean prior to descibe functions.  Errors encountered \n",
    "- duplicate index caused error (this was due to concatenating dataframes)\n",
    "- NaN causes KeyError messages.  Columns must be clean !\n",
    "- Pandas takes care of these automatically ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T14:24:57.509324Z",
     "start_time": "2019-09-18T14:24:57.499412Z"
    }
   },
   "outputs": [],
   "source": [
    "# NaN report\n",
    "def nan_report(df) :\n",
    "    for c in df.columns :\n",
    "        print(\"{} {}\".format(c, df[c].null_count))\n",
    "\n",
    "nan_report(loan_rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SnapML\n",
    "\n",
    "X = loan_norm_rdfloan_rdf2.to_pandas()\n",
    "y = loan_rdf['default'].to_pandas()\n",
    "\n",
    "from pai4sk.linear_model import Ridge as LRSNAP\n",
    "clf = LRSNAP(alpha=1.0)\n",
    "clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE\n",
    "\n",
    "\n",
    "#Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
    "#      normalize=False, random_state=None, solver='auto', tol=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "350.199px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
